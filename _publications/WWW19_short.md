---
title: "Semantic Hilbert Space for Text Representation Learning"
collection: publications
permalink: /publication/WWW19_short
excerpt: 'We introduce the Semantic Hilbert Space to model different levels of linguistics units, and build a complex-valued network to implement the framework for text classification.'
date: 2019-03
venue: 'The Web Conference 2019 (WWW2019)'
paperurl: 'http://academicpages.github.io/files/paper3.pdf'
citation: 'Benyou Wang, Qiuchi Li, Massimo Melucci, and Dawei Song. Semantic Hilbert Space for Text Representation Learning (2019). &quot; .&quot; <i>The Web Conference 2018</i>.'
---
Capturing the meaning of sentences has long been a challenging task. Current models tend to apply linear combinations of word features to conduct semantic composition for a bigger-granularity units e.g. phrase, sentence and documents. However, the semantic linearity does not always hold in human language. For instance, the meaning of the phrase "ivory tower" can not be deduced by linearly combining the meanings of "ivory" and "tower".  To address this issue,  we propose a new framework that models different levels of semantic units (e.g. sememe, word, sentence and semantic abstraction) on a single Semantic Hilbert Space, which naturally admits a non-linear semantic composition by means of a complex-valued vector word representation. 
%inspired by recent findings that human language understanding could exhibit some quantum-like phenomena. 
An end-to-end neural network is proposed to implement the framework in the text classification task, and evaluation results on six benchmarking text classification datasets demonstrate the effectiveness, robustness and self-explanation power of the proposed model. Furthermore, intuitive case studies are conducted to help end users to understand how the framework works.

[Download paper here](http://qiuchili.github.io/files/WWW19_short.pdf)
